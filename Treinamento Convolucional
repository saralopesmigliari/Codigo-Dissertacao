# -*- coding: utf-8 -*-
"""Arquitetura Treinamento com Convolucional

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RYmcVLm-nTMbCIqVQHTJ1O06HblUn_Kp
"""

from google.colab import drive
drive.mount('/content/drive')

from PIL import Image
from os import listdir
from os.path import isdir
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical
from keras.layers import Dense, Activation, Dropout, Reshape, Permute, Conv2D, MaxPooling2D, Conv3D
#from keras.layers.convolutional import Conv2D
#from keras.layers.convolutional import Conv3D
#from keras.layers.convolutional import MaxPooling2D
from keras.models import Sequential
from tensorflow.keras import layers
from tensorflow.keras import models
from tensorflow.keras import optimizers
from sklearn.metrics import classification_report
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

# LEITURA DA IMAGEM:

def select_image(filename):
    image = Image.open(filename)
    image = image.convert('RGB')
    image = image.resize((299,299))
    return np.asarray(image)

# CARREGAMENTO DAS CLASSES:

def load_class(diretorio, classe, imagens, labels):
    for filename in listdir(diretorio):
        path = diretorio + filename
        try:
            imagens.append(select_image(path))
            labels.append(classe)
        except:
            print("Erro ao ler imagem {}".format(path))
    return imagens, labels

# SUBDIVISÃO DE DIRETÓRIO

def select_data_set(diretorio):
    imagens = list()
    labels = list()
    for subdir in listdir(diretorio):
        path = diretorio + subdir + '/'
        if not isdir(path):
            continue
        imagens, labels = load_class(path, subdir, imagens, labels)
    return imagens, labels

# TRANSFORMANDO IMAGENS E CLASSES EM NUMPY E ARRAY:

base_dataset = "/content/drive/MyDrive/PROJETO_DANILO/IMAGEM FOLHAS - SEGMENTADO/Binary/"
imagens, labels = select_data_set(base_dataset)
imagens = np.array(imagens) / 255.0
labels = np.array(labels)

# CATEGORIZAÇÃO

lb = LabelEncoder()
labels = lb.fit_transform(labels)
labels = to_categorical(labels)

# DEFINIÇÃO DOS PARÂMETROS DE RNA

batch_size = 32 #Tamanho dos pacotes de imagem para o RNA conseguir treinar
input_shape = (299, 299, 3) #Formato da imagem
random_state = 42
alpha       = 0.01 #Taxa de aprendizado
epoch       = 100

"""**Definição dos callbacks:**

Funções que ajudam o modelo que está sendo utilizado a aprimorar os resultados.
"""

filepath="transferlearning_weights.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')

callbacks = [checkpoint]

# SPLIT

(trainX, testX, trainY, testY) = train_test_split(imagens, labels, test_size=0.2, stratify=labels, random_state=random_state)

train_datagen = ImageDataGenerator(rotation_range=20,zoom_range=0.2)
train_datagen.fit(trainX)
data_aug = train_datagen.flow(trainX, trainY, batch_size=batch_size)

# Definição da estrutura da CNN (aplicação de camadas):

model = Sequential()
model.add(Conv2D(128, (5, 5), input_shape=(299, 299, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(20, 20)))
model.add(Conv2D(32, (5, 5), input_shape=(299, 299, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(10, 10)))
model.add(layers.GlobalAveragePooling2D())
model.add(layers.BatchNormalization())
model.add(layers.Flatten())
#model.add(layers.Dense(4000, activation='relu'))
#model.add(layers.Dense(3000, activation='relu'))
#model.add(layers.Dense(2000, activation='relu'))
#model.add(layers.Dense(1000, activation='relu'))
#model.add(layers.Dense(500, activation='relu'))
model.add(layers.Dense(250, activation='relu'))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dropout(0.3))
model.add(layers.Dense(6, activation='softmax'))

model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['acc'])

# INICIO DO TREINAMENTO

history = model.fit_generator(
                              data_aug,
                              steps_per_epoch=len(trainX)//batch_size,
                              validation_data=(testX, testY),
                              validation_steps=len(testX)//batch_size,
                              callbacks=callbacks,
                              epochs=epoch)

model.save_weights(filepath)
model.save('ModeloTreinado.h5')

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

plt.plot(history.history['acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
plt.plot(history.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

from sklearn.metrics import confusion_matrix
pred = model.predict(testX)
pred = np.argmax(pred,axis = 1)
y_true = np.argmax(testY, axis = 1)

#Colocar a quantidade de classe contando a partir do 0 até a quantidade que tem

cm = confusion_matrix(y_true, pred)
total = sum(sum(cm))
scores = model.evaluate(testX, testY, verbose=0)
print("\nAcurácia: %.2f%%" % (scores[1]*100))
from mlxtend.plotting import plot_confusion_matrix
fig, ax = plot_confusion_matrix(conf_mat=cm, figsize=(5, 5))
plt.show()

scores = model.evaluate(testX, testY, verbose=0)
print("\nacc: %.2f%%" % (scores[1]*100))

print(classification_report(y_true, pred))
